{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc9c516d",
   "metadata": {},
   "source": [
    "# How to perform experiments with damast\n",
    "\n",
    "One of the main motivation of this library is to facilitate the development and evaluation of Machine-Learning models.\n",
    "Hence, 'damast' offers a mini-framework and API to simplify the development of machine learning models.\n",
    "This requires a 'hopefully' minimal set of constraints - as what is envisioned by the authors of this library - so that researchers and ML-starters have lower entry barrier into running machine learning.\n",
    "\n",
    "That being said, we give an example here on a minimal experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6674a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-31 12:40:42.548991: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-31 12:40:42.678089: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-03-31 12:40:43.256819: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-31 12:40:43.256866: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-31 12:40:43.256870: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-03-31 12:40:49.455285: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-03-31 12:40:49.455356: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (homeland): /proc/driver/nvidia/version does not exist\n",
      "2023-03-31 12:40:49.457630: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# The list of modules used for this example\n",
    "from collections import OrderedDict\n",
    "from typing import List, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "# For performance reasons the underlying data handling library is 'vaex'\n",
    "import vaex\n",
    "\n",
    "# The current development has focused on a keras+tensorflow based Machine Learning setup\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "import damast\n",
    "# You can define custom units to annotate data, but otherwise astropy units will be used\n",
    "from damast.core.units import units\n",
    "# Data ranges can be defined as list, or marked with a lower-bound (min), upper-bound (max)\n",
    "from damast.core.datarange import MinMax, CyclicMinMax\n",
    "# The AnnotatedDataFrame combines a data specification and actual 'numeric' data\n",
    "from damast.core.dataframe import AnnotatedDataFrame\n",
    "# An AnnotatedDataFrame contains MetaData to describe the data\n",
    "from damast.core.metadata import MetaData\n",
    "\n",
    "# Data processing is centered around a DataProcessingPipeline which consists of multiple PipelineElement being run\n",
    "# in sequence\n",
    "from damast.core.dataprocessing import DataProcessingPipeline, PipelineElement\n",
    "\n",
    "\n",
    "# To allow the machine learning process to be simplified, we offer a 'BaseModel' that should be inherited from\n",
    "from damast.ml.models.base import BaseModel\n",
    "\n",
    "# The experiment setup\n",
    "from damast.ml.experiments import Experiment, LearningTask, ForecastTask, ModelInstanceDescription, TrainingParameters\n",
    "\n",
    "# Allow to generate data for this particular example that uses data from the maritime domain\n",
    "from damast.domains.maritime.ais.data_generator import AISTestData, AISTestDataSpec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2ff3ae",
   "metadata": {},
   "source": [
    "To illustrate a full experiment, we require a data processing pipeline to be set up. This pipeline will extract all those features, that are necessary to train the Machine Learning model(s). The pipeline will run transformations on the data, as provided here by a LatLonTransformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "395f3a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatLonTransformer(PipelineElement):\n",
    "    \"\"\"\n",
    "    The LatLonTransformer will consume a lat(itude) and a lon(gitude) column and perform\n",
    "    cyclic normalization. It will add four column to a dataframe, namely lat_x, lat_y, lon_x, lon_y.\n",
    "    \"\"\"\n",
    "    @damast.core.describe(\"Lat/Lon cyclic transformation\")\n",
    "    @damast.core.input({\n",
    "        \"lat\": {\"unit\": units.deg},\n",
    "        \"lon\": {\"unit\": units.deg}\n",
    "    })\n",
    "    @damast.core.output({\n",
    "        \"lat_x\": {\"value_range\": MinMax(-1.0, 1.0)},\n",
    "        \"lat_y\": {\"value_range\": MinMax(-1.0, 1.0)},\n",
    "        \"lon_x\": {\"value_range\": MinMax(-1.0, 1.0)},\n",
    "        \"lon_y\": {\"value_range\": MinMax(-1.0, 1.0)}\n",
    "    })\n",
    "    def transform(self, df: AnnotatedDataFrame) -> AnnotatedDataFrame:        \n",
    "        lat_cyclic_transformer = vaex.ml.CycleTransformer(features=[\"lat\"], n=180.0)\n",
    "        lon_cyclic_transformer = vaex.ml.CycleTransformer(features=[\"lon\"], n=360.0)\n",
    "\n",
    "        _df = lat_cyclic_transformer.fit_transform(df=df)\n",
    "        _df = lon_cyclic_transformer.fit_transform(df=_df)\n",
    "        df._dataframe = _df\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a8163e",
   "metadata": {},
   "source": [
    "The selected example model here, will require the above listed features as input - and will likewise a likewise shaped output for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7720235a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baseline(BaseModel):\n",
    "    \"\"\"\n",
    "    This is a placeholder ML model that illustrates the minimal\n",
    "    requirements.\n",
    "    \"\"\"\n",
    "    input_specs = OrderedDict({\n",
    "        \"lat_x\": {\"length\": 1},\n",
    "        \"lat_y\": {\"length\": 1},\n",
    "        \"lon_x\": {\"length\": 1},\n",
    "        \"lon_y\": {\"length\": 1}\n",
    "    })\n",
    "\n",
    "    output_specs = OrderedDict({\n",
    "        \"lat_x\": {\"length\": 1},\n",
    "        \"lat_y\": {\"length\": 1},\n",
    "        \"lon_x\": {\"length\": 1},\n",
    "        \"lon_y\": {\"length\": 1}\n",
    "    })\n",
    "\n",
    "    def __init__(self,\n",
    "                 name: str,\n",
    "                 features: List[str],\n",
    "                 timeline_length: int,\n",
    "                 output_dir: Path,\n",
    "                 targets: Optional[List[str]] = None):\n",
    "        self.timeline_length = timeline_length\n",
    "\n",
    "        super().__init__(name=name,\n",
    "                         output_dir=output_dir,\n",
    "                         features=features,\n",
    "                         targets=targets)\n",
    "\n",
    "    def _init_model(self):\n",
    "        features_width = len(self.features)\n",
    "        targets_width = len(self.targets)\n",
    "\n",
    "        self.model = tf.keras.models.Sequential([\n",
    "            keras.layers.Flatten(input_shape=[self.timeline_length, features_width]),\n",
    "            keras.layers.Dense(targets_width)\n",
    "        ])\n",
    "\n",
    "\n",
    "class BaselineA(Baseline):\n",
    "    \"\"\"Placeholder Model to illustrate the use of multiple models\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class BaselineB(Baseline):\n",
    "    \"\"\"Placeholder Model to illustrate the use of multiple models\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce825648",
   "metadata": {},
   "source": [
    "This example operates with synthetic, i.e. automatically generated data which is specific to the maritime domain.\n",
    "You will see a previous of the first 10 columns when running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3a50353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>#                            </th><th style=\"text-align: right;\">      mmsi</th><th style=\"text-align: right;\">     lon</th><th style=\"text-align: right;\">    lat</th><th>date_time_utc      </th><th style=\"text-align: right;\">     sog</th><th style=\"text-align: right;\">      cog</th><th style=\"text-align: right;\">  true_heading</th><th style=\"text-align: right;\">  nav_status</th><th style=\"text-align: right;\">  rot</th><th style=\"text-align: right;\">  message_nr</th><th>source  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td><i style='opacity: 0.6'>0</i></td><td style=\"text-align: right;\">1277812958</td><td style=\"text-align: right;\">-156.102</td><td style=\"text-align: right;\">66.8031</td><td>1983-11-14 09:14:28</td><td style=\"text-align: right;\">0.787616</td><td style=\"text-align: right;\">-0.174439</td><td style=\"text-align: right;\">     -0.11852 </td><td style=\"text-align: right;\">           7</td><td style=\"text-align: right;\">    0</td><td style=\"text-align: right;\">           2</td><td>s       </td></tr>\n",
       "<tr><td><i style='opacity: 0.6'>1</i></td><td style=\"text-align: right;\">1277812958</td><td style=\"text-align: right;\">-156.094</td><td style=\"text-align: right;\">66.817 </td><td>1983-11-14 09:15:20</td><td style=\"text-align: right;\">1.25607 </td><td style=\"text-align: right;\">-0.588664</td><td style=\"text-align: right;\">     -0.546531</td><td style=\"text-align: right;\">           0</td><td style=\"text-align: right;\">    0</td><td style=\"text-align: right;\">           2</td><td>g       </td></tr>\n",
       "<tr><td><i style='opacity: 0.6'>2</i></td><td style=\"text-align: right;\">1277812958</td><td style=\"text-align: right;\">-156.108</td><td style=\"text-align: right;\">66.8583</td><td>1983-11-14 09:16:39</td><td style=\"text-align: right;\">1.85139 </td><td style=\"text-align: right;\">-1.06371 </td><td style=\"text-align: right;\">     -1.04555 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">    0</td><td style=\"text-align: right;\">           2</td><td>g       </td></tr>\n",
       "<tr><td><i style='opacity: 0.6'>3</i></td><td style=\"text-align: right;\">1277812958</td><td style=\"text-align: right;\">-156.127</td><td style=\"text-align: right;\">66.8768</td><td>1983-11-14 09:18:31</td><td style=\"text-align: right;\">1.49558 </td><td style=\"text-align: right;\">-1.14412 </td><td style=\"text-align: right;\">     -1.13982 </td><td style=\"text-align: right;\">           7</td><td style=\"text-align: right;\">    0</td><td style=\"text-align: right;\">           2</td><td>s       </td></tr>\n",
       "<tr><td><i style='opacity: 0.6'>4</i></td><td style=\"text-align: right;\">1277812958</td><td style=\"text-align: right;\">-156.167</td><td style=\"text-align: right;\">66.9099</td><td>1983-11-14 09:19:05</td><td style=\"text-align: right;\">3.38864 </td><td style=\"text-align: right;\">-1.52297 </td><td style=\"text-align: right;\">     -1.51993 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">    0</td><td style=\"text-align: right;\">           2</td><td>s       </td></tr>\n",
       "<tr><td><i style='opacity: 0.6'>5</i></td><td style=\"text-align: right;\">1277812958</td><td style=\"text-align: right;\">-156.195</td><td style=\"text-align: right;\">66.8728</td><td>1983-11-14 09:19:38</td><td style=\"text-align: right;\">2.51732 </td><td style=\"text-align: right;\">-1.50282 </td><td style=\"text-align: right;\">     -1.42802 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">    0</td><td style=\"text-align: right;\">           2</td><td>s       </td></tr>\n",
       "<tr><td><i style='opacity: 0.6'>6</i></td><td style=\"text-align: right;\">1277812958</td><td style=\"text-align: right;\">-156.181</td><td style=\"text-align: right;\">66.8417</td><td>1983-11-14 09:20:50</td><td style=\"text-align: right;\">2.90567 </td><td style=\"text-align: right;\">-1.00414 </td><td style=\"text-align: right;\">     -0.985169</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">    0</td><td style=\"text-align: right;\">           2</td><td>s       </td></tr>\n",
       "<tr><td><i style='opacity: 0.6'>7</i></td><td style=\"text-align: right;\">1277812958</td><td style=\"text-align: right;\">-156.169</td><td style=\"text-align: right;\">66.8574</td><td>1983-11-14 09:22:16</td><td style=\"text-align: right;\">1.41212 </td><td style=\"text-align: right;\">-0.658907</td><td style=\"text-align: right;\">     -0.646031</td><td style=\"text-align: right;\">           7</td><td style=\"text-align: right;\">    0</td><td style=\"text-align: right;\">           2</td><td>g       </td></tr>\n",
       "<tr><td><i style='opacity: 0.6'>8</i></td><td style=\"text-align: right;\">1277812958</td><td style=\"text-align: right;\">-156.14 </td><td style=\"text-align: right;\">66.8957</td><td>1983-11-14 09:23:02</td><td style=\"text-align: right;\">2.19664 </td><td style=\"text-align: right;\">-0.855472</td><td style=\"text-align: right;\">     -0.759963</td><td style=\"text-align: right;\">           7</td><td style=\"text-align: right;\">    0</td><td style=\"text-align: right;\">           2</td><td>s       </td></tr>\n",
       "<tr><td><i style='opacity: 0.6'>9</i></td><td style=\"text-align: right;\">1277812958</td><td style=\"text-align: right;\">-156.156</td><td style=\"text-align: right;\">66.904 </td><td>1983-11-14 09:23:32</td><td style=\"text-align: right;\">2.16217 </td><td style=\"text-align: right;\">-0.881218</td><td style=\"text-align: right;\">     -0.852941</td><td style=\"text-align: right;\">           0</td><td style=\"text-align: right;\">    0</td><td style=\"text-align: right;\">           2</td><td>s       </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "  #        mmsi       lon      lat  date_time_utc             sog        cog    true_heading    nav_status    rot    message_nr  source\n",
       "  0  1277812958  -156.102  66.8031  1983-11-14 09:14:28  0.787616  -0.174439       -0.11852              7      0             2  s\n",
       "  1  1277812958  -156.094  66.817   1983-11-14 09:15:20  1.25607   -0.588664       -0.546531             0      0             2  g\n",
       "  2  1277812958  -156.108  66.8583  1983-11-14 09:16:39  1.85139   -1.06371        -1.04555              1      0             2  g\n",
       "  3  1277812958  -156.127  66.8768  1983-11-14 09:18:31  1.49558   -1.14412        -1.13982              7      0             2  s\n",
       "  4  1277812958  -156.167  66.9099  1983-11-14 09:19:05  3.38864   -1.52297        -1.51993              1      0             2  s\n",
       "  5  1277812958  -156.195  66.8728  1983-11-14 09:19:38  2.51732   -1.50282        -1.42802              1      0             2  s\n",
       "  6  1277812958  -156.181  66.8417  1983-11-14 09:20:50  2.90567   -1.00414        -0.985169             1      0             2  s\n",
       "  7  1277812958  -156.169  66.8574  1983-11-14 09:22:16  1.41212   -0.658907       -0.646031             7      0             2  g\n",
       "  8  1277812958  -156.14   66.8957  1983-11-14 09:23:02  2.19664   -0.855472       -0.759963             7      0             2  s\n",
       "  9  1277812958  -156.156  66.904   1983-11-14 09:23:32  2.16217   -0.881218       -0.852941             0      0             2  s"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "tmp_path = Path(tempfile.gettempdir()) / \"test-output-ais_preparation\"\n",
    "if tmp_path.exists():\n",
    "    shutil.rmtree(tmp_path)\n",
    "tmp_path.mkdir(parents=True)\n",
    "\n",
    "pipeline = DataProcessingPipeline(name=\"ais_preparation\",\n",
    "                                  base_dir=tmp_path) \\\n",
    "    .add(\"cyclic\", LatLonTransformer())\n",
    "features = [\"lat_x\", \"lat_y\", \"lon_x\", \"lon_y\"]\n",
    "\n",
    "data = AISTestData(1000)\n",
    "adf = AnnotatedDataFrame(dataframe=data.dataframe,\n",
    "                         metadata=MetaData.from_dict(data=AISTestDataSpec.copy()))\n",
    "dataset_filename = tmp_path / \"test.hdf5\"\n",
    "adf.save(filename=dataset_filename)\n",
    "\n",
    "adf.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d139d6f7",
   "metadata": {},
   "source": [
    "A central idea to the experiment framework lies in providing a means for a consistent input and output to perform experiments. Hence, define a LearningTask (here: ForecastTask) that collects the learning parameters that define this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f1f47cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_task = ForecastTask(\n",
    "    label=\"forecast-ais-short-sequence\",\n",
    "    pipeline=pipeline, features=features,\n",
    "    models=[ModelInstanceDescription(BaselineA, {}),\n",
    "            ModelInstanceDescription(BaselineB, {}),\n",
    "            ],\n",
    "    group_column=\"mmsi\",\n",
    "    sequence_length=5,\n",
    "    forecast_length=1,\n",
    "    training_parameters=TrainingParameters(epochs=1,\n",
    "                                           validation_steps=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb32d5a",
   "metadata": {},
   "source": [
    "The actual experimentation takes a single LearningTask as input and it will output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48163133",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:damast:Recommended steps_per_epoch=53490.0\n",
      "INFO:damast:Recommended steps_per_epoch=53490.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 20)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 84        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 84\n",
      "Trainable params: 84\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "10/10 [==============================] - 1s 46ms/step - loss: 0.6682 - mse: 0.6682 - val_loss: 0.0501 - val_mse: 0.0501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:damast:Recommended steps_per_epoch=53490.0\n",
      "INFO:damast:Recommended steps_per_epoch=53490.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_1 (Flatten)         (None, 20)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 84        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 84\n",
      "Trainable params: 84\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "10/10 [==============================] - 1s 40ms/step - loss: 0.5518 - mse: 0.5518 - val_loss: 0.6769 - val_mse: 0.6769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:damast:Recommended steps_per_epoch=53490.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 2\n",
      "evaluation:\n",
      "  BaselineA-forecast-ais-short-sequence:\n",
      "    loss: 0.020285729318857193\n",
      "    mse: 0.020285729318857193\n",
      "  BaselineB-forecast-ais-short-sequence:\n",
      "    loss: 0.1729123294353485\n",
      "    mse: 0.1729123294353485\n",
      "evaluation_steps: 1\n",
      "input_data: /tmp/test-output-ais_preparation/test.hdf5\n",
      "label: damast-ml-experiment\n",
      "learning_task:\n",
      "  class_name: ForecastTask\n",
      "  features: &id001\n",
      "  - lat_x\n",
      "  - lat_y\n",
      "  - lon_x\n",
      "  - lon_y\n",
      "  forecast_length: 1\n",
      "  group_column: mmsi\n",
      "  label: forecast-ais-short-sequence\n",
      "  models:\n",
      "  - class_name: BaselineA\n",
      "    module_name: __main__\n",
      "    parameters: {}\n",
      "  - class_name: BaselineB\n",
      "    module_name: __main__\n",
      "    parameters: {}\n",
      "  module_name: damast.ml.experiments\n",
      "  pipeline:\n",
      "    base_dir: /tmp/test-output-ais_preparation\n",
      "    name: ais_preparation\n",
      "    steps:\n",
      "    - - cyclic\n",
      "      - class_name: LatLonTransformer\n",
      "        module_name: __main__\n",
      "        name_mappings: {}\n",
      "  sequence_length: 5\n",
      "  targets: *id001\n",
      "  training_parameters:\n",
      "    epochs: 1\n",
      "    learning_rate: 0.1\n",
      "    loss_function: mse\n",
      "    steps_per_epoch: 10\n",
      "    validation_steps: 1\n",
      "output_directory: /tmp/test-output-ais_preparation\n",
      "split_data_ratios:\n",
      "- 1.6\n",
      "- 0.2\n",
      "- 0.2\n",
      "timestamp: 20230331-104105\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiment = Experiment(learning_task=forecast_task,\n",
    "                        input_data=dataset_filename,\n",
    "                        output_directory=tmp_path)\n",
    "report = experiment.run()\n",
    "    \n",
    "with open(report, \"r\") as f:\n",
    "    print(f.read())        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e524aa52",
   "metadata": {},
   "source": [
    "The outputs of an experiment are collected inside a dedicated (timestamped) folder. This folder will also contain a subfolder for each of the parametrized models that defines a LearningTask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe356d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last experiment in:  /tmp/test-output-ais_preparation/20230331-104103-damast-ml-experiment\n",
      "Contents:\n",
      "\n",
      "/tmp/test-output-ais_preparation/20230331-104103-damast-ml-experiment/BaselineA-forecast-ais-short-sequence\n",
      "/tmp/test-output-ais_preparation/20230331-104103-damast-ml-experiment/BaselineB-forecast-ais-short-sequence\n",
      "/tmp/test-output-ais_preparation/20230331-104103-damast-ml-experiment/experiment-report.yaml\n"
     ]
    }
   ],
   "source": [
    "last_experiments = sorted([str(f) for f in Path(experiment.output_directory).glob(pattern=\"*\") if f.is_dir()])\n",
    "print(\"Last experiment in: \", last_experiments[-1])\n",
    "\n",
    "experiment_folder = sorted([str(f) for f in Path(last_experiments[-1]).glob(pattern=\"*\")])\n",
    "file_listing = '\\n'.join(experiment_folder)\n",
    "print(\"Contents:\\n\")\n",
    "print(file_listing)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
