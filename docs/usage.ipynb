{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage\n",
    "We start by exploring the data-processing pipeline part of `DAMAST`.\n",
    "We consider a manufactured dataset of Automatic Identification System (AIS) messages.\n",
    "The data is generated for 150 boats, where the minimal length of a trajectory is 30 messages, and the maximal length is 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install damast\n",
    "\n",
    "import polars\n",
    "import damast.domains.maritime.ais.data_generator as generator\n",
    "\n",
    "data = generator.AISTestData(number_of_trajectories=1000, min_length=25, max_length=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is stored in a [polars.LazyFrame](https://docs.pola.rs/api/python/stable/reference/lazyframe/index.html), and we can inspect the first and last 5 messages in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of 11 columns, which we will go through in detail.\n",
    "\n",
    "## Data-specification\n",
    "The Maritime Mobile Service Identity (MMSI) used to identify a ship. It *should* be a 9 digit number whose first integer should be between 2 and 7.\n",
    "The data we have generated should contain some invalid numbers. Let us inspect these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from damast.domains.maritime.data_specification import MMSI\n",
    "df = data.dataframe\n",
    "invalid_mmsis = df.filter((polars.col('mmsi') < MMSI.min_value) | (polars.col('mmsi') > MMSI.max_value))\n",
    "invalid_mmsis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before sending this data to a machine learning algorithm, one would have to filter out invalid data.\n",
    "We can do this by creating a `damast.core.DataSpecification` describing what valid output we would like in our data-frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from damast.core import DataSpecification, MinMax\n",
    "mmsi_spec = DataSpecification(name=\"mmsi\", description=\"Maritime Mobile Service Identity\", representation_type=int,\n",
    "                              value_range=MinMax(MMSI.min_value, MMSI.max_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have here described what data this column is supposed to describe, how the data is represented in Python, and its minimum and maximum range.\n",
    "Next, we create a `damast.core.MetaData` object that we can apply to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from damast.core import MetaData,ValidationMode\n",
    "metadata = MetaData([mmsi_spec])\n",
    "metadata.apply(df.lazy(), ValidationMode.UPDATE_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we do not want to do this process manually per row. Therefore, we can create a `DataSpecification` per row, and let the `damast.core.AnnotatedDataFrame` handle the validation of the data. We can choose between three ways of handling the input data with metadata, we can either use:\n",
    "- `ValidationMode.READONLY`: Reads in the data, checks it against the meta-data and throws an error if the data does not adhere to the data-specification.\n",
    "- `ValidationMode.UPDATE_METADATA`: Update the metadata based on the input in the annotated data-frame. This might change the representation type, column name and valid rages of the data.\n",
    "- `ValidationMode.UPDATE_DATA`: Update data so that it adheres to the meta-data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from damast.core.metadata import DataCategory\n",
    "from damast.core.dataframe import AnnotatedDataFrame\n",
    "dataspec = {\n",
    "    \"annotations\": {\"comment\": \"This is a autogenerated test data set\"},\n",
    "    \"columns\": [\n",
    "        {\"name\": \"mmsi\", \"is_optional\": False, \"category\": DataCategory.STATIC,\n",
    "         \"value_range\":{\"MinMax\": {\"min\": MMSI.min_value, \"max\": MMSI.max_value}}},\n",
    "        {\"name\": \"lon\", \"is_optional\": False, \"unit\": \"deg\", \"category\": DataCategory.DYNAMIC},\n",
    "        {\"name\": \"lat\", \"is_optional\": False, \"unit\": \"deg\", \"category\": DataCategory.DYNAMIC},\n",
    "        {\"name\": \"date_time_utc\", \"is_optional\": False, \"category\": DataCategory.DYNAMIC},\n",
    "        {\"name\": \"sog\", \"is_optional\": False, \"category\": DataCategory.DYNAMIC},\n",
    "        {\"name\": \"cog\", \"is_optional\": False, \"category\": DataCategory.DYNAMIC},\n",
    "        {\"name\": \"true_heading\", \"is_optional\": False, \"category\": DataCategory.DYNAMIC},\n",
    "        {\"name\": \"nav_status\", \"is_optional\": False, \"category\": DataCategory.DYNAMIC},\n",
    "        {\"name\": \"rot\", \"is_optional\": False, \"category\": DataCategory.DYNAMIC},\n",
    "        {\"name\": \"message_nr\", \"is_optional\": False, \"category\": DataCategory.DYNAMIC},\n",
    "        {\"name\": \"source\", \"is_optional\": False, \"category\": DataCategory.DYNAMIC},\n",
    "    ]\n",
    "}\n",
    "metadata = MetaData.from_dict(dataspec)\n",
    "data = generator.AISTestData(number_of_trajectories=1000, min_length=25, max_length=300)\n",
    "adf = AnnotatedDataFrame(data.dataframe, metadata, validation_mode=ValidationMode.UPDATE_DATA)\n",
    "adf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data-processing\n",
    "Say we want to repeat this process on any data-set we read in. Then, we should create a `damast.core.dataprocessing.DataProcessingPipeline`.\n",
    "A pipeline consists of pipeline-elements, that is a set of transformations on the original dataset.\n",
    "We start by creating a Pipeline-element that drops all rows missing an `\"mmsi\"` entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from damast.data_handling.transformers.filters import DropMissingOrNan\n",
    "from damast.core.dataprocessing import DataProcessingPipeline\n",
    "pipeline = DataProcessingPipeline(name=\"Remove missing MMSI columns\",\n",
    "                                  base_dir=\"./output_dir\",\n",
    "                                  inplace_transformation=True)\n",
    "pipeline.add(name=\"Remove MMSI column\",\n",
    "             transformer=DropMissingOrNan(),\n",
    "             name_mappings={\"x\": \"mmsi\"})\n",
    "\n",
    "transformed_adf = pipeline.transform(adf)\n",
    "transformed_adf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_adf.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pipelines with multiple input sources\n",
    "When multiple input sources exist and should be merged, a join operator (transformer) can be designed.\n",
    "The join can, but must not necessarily involve two pipelines as illustrated in the following.\n",
    "The pipeline is named using the 'data_source' argument, but this is optiona\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import damast\n",
    "from damast.core.transformations import PipelineElement\n",
    "\n",
    "class JoinByTime(PipelineElement):                                                                                                                                                                             \n",
    "    @damast.core.describe(\"Join data by timestamp\")                                                                                        \n",
    "    @damast.core.input({                                                                                                          \n",
    "                           \"timestamp\": {},                                                                                       \n",
    "                           \"lon\": {},                                                                                             \n",
    "                           \"lat\": {},                                                                                             \n",
    "                       })                                                                                                         \n",
    "    @damast.core.input({                                                                                                          \n",
    "                            \"timestamp\": {},                                                                                      \n",
    "                            \"lat\": {},                                                                                            \n",
    "                            \"lon\": {}                                                                                             \n",
    "                        }, label='other'                                                                                          \n",
    "    )                                                                                                                             \n",
    "    @damast.core.output({})                                                                                                       \n",
    "    def transform(self, df: AnnotatedDataFrame, other: AnnotatedDataFrame) -> AnnotatedDataFrame:                                 \n",
    "        other_timestamp = self.get_name('timestamp', datasource='other')                                                          \n",
    "        df_timestamp = self.get_name('timestamp')                                                                                 \n",
    "                                                                                                                                  \n",
    "        df._dataframe = df.join(other._dataframe, left_on=df_timestamp, right_on=other_timestamp)                                 \n",
    "        return df\n",
    "\n",
    "event_time = data.dataframe.drop_nulls().select(polars.col('date_time_utc')).item(0,0)\n",
    "events_dataframe = polars.from_dict({'latitude': [40.0, 40.1], 'longitude': [10.0,10.2], 'timestamp': [event_time, event_time], 'event_type': [\"accident\", \"accident\"]})\n",
    "events_metadata = AnnotatedDataFrame.infer_annotation(events_dataframe)\n",
    "events_adf = AnnotatedDataFrame(events_dataframe, metadata=events_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from damast.data_handling.transformers.cycle_transformer import CycleTransformer\n",
    "\n",
    "events_pipeline = DataProcessingPipeline(name=\"events\",\n",
    "                                  base_dir=\"./output_dir\") \\\n",
    "    .add(\"lat_cycle_transform\", CycleTransformer(n=180), name_mappings={'x': 'latitude'}) \\\n",
    "    .add(\"lon_cycle_transform\", CycleTransformer(n=90), name_mappings={'x': 'longitude'}) \\\n",
    "\n",
    "pipeline = DataProcessingPipeline(name=\"ais_events_merge\",\n",
    "                                  base_dir=\"./output_dir\") \\\n",
    "    .add(\"lat_cycle_transform\", CycleTransformer(n=180), name_mappings={'x': 'lat'}) \\\n",
    "    .add(\"lon_cycle_transform\", CycleTransformer(n=90), name_mappings={'x': 'lon'}) \\\n",
    "    .join(\"events\", data_source=events_pipeline, operator=JoinByTime(),\n",
    "              name_mappings = {\n",
    "                  'df': {\n",
    "                      \"timestamp\": \"date_time_utc\",\n",
    "                  },\n",
    "                  'other': {\n",
    "                      \"timestamp\": \"timestamp\",\n",
    "                      \"lon\": \"longitude\",\n",
    "                      \"lat\": \"latitude\"\n",
    "                  }\n",
    "              },\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the pipeline, all required datasource aka inputs need to be provided as arguments.\n",
    "While the default input is 'df', the datasource for the 'join' operator requires to be provided via\n",
    " the keyword of the same name, here 'events'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_adf = pipeline.transform(df=adf, events=events_adf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_adf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_adf.head(10).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "damast-test-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
